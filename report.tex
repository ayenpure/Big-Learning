\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{cite}

\title{Big Learning Seminar Report}
\author{Abhishek Yenpure}
\date{Spring 2019}

\begin{document}

\maketitle

\section{Introduction}
%
The intent of this report is to discuss the learnings of the seminar.
%
The seminar focused on the area of intersection of two fields, Big Data and Deep Learning.
%
In this section I introduce the readers to the broad themes of these two fields.
%
This introduction helps build a foundation that helps readers to better understand the
sections in which describes works discussed in the seminar. 

Big Data and Deep Learning are among the most popular research fields.
%

%
\subsection{Big Data}
"Big Data" usually referes to ways to analyze and extract information form very
data that is too large.
%
Big Data processing has become increasingly important due to the amounts of data
that is being generated on a daily basis.
%
Processing such large data is a challange because traditonal approaches often fail
to do so efficiently.
%
Big data is characterized by three Vs, namely Volume, Velocity, and Variety.
%
Each of these three Vs represents different challanges involved in the processing
of big data.
%
\begin{itemize}
\item \textbf{Volume:}
%
This aspect of big data alludes to the magnitudes of data that needs to be stored
and processed.
%
The fact that 90\% of the world's data was generated in the past two years gives
us an idea of why dealing with this aspect of big data is necessary.
%
To be able to make the most use of available data we need to process huge volumes
of data, most times with a time constraint.
%
This further aggravates the challange.

\item \textbf{Velocity:}
%
This aspect of big data alluded to the rate at which data or content is created.
%
Each one of the billion of people who use the world wide web contributes to data
that is created, either directly or indirectly.
%
The constant stream of new data makes it very difficult to process data efficiently
to make the most relevant analysis.

\item \textbf{Variety:}
%
This aspect of big data alludes to diffrent forms of data.
%
In this context data can exist in many forms like images, text, videos, etc.
%
To process each of these types of data require special considertions and techniques
cannot be interchangably applied.
%
This requries a change in the traditional approached for storing, retrieving, and 
analyzing related data.

\end{itemize}
%
\subsection{Deep Learning}
Deep learning encapsulates methods of machine learning which uses multiple layers
to extract features from data.
%
Each of these layers progressively extracts higher level features.
%
This broad class of methods include deep neural networks, deep belief networks,
convolutional neural networks, and recurrent neural networks.
%
These techniques are widely used in an increasingle large number of domains like
computer vision, speech recognition, etc.
%
To learn characteristic and hidden patterns in data it Deep Learning techniques
present a huge potential.
\\
Hence, in the age of Big Data, to extract hidden patterns/information make
Deep Learning a viable candiate.
For the scope of this report I have choosen to summerize the following works.

\begin{itemize}
\item Big Data Deep Learning: Challenges and Perspectives~\cite{chen2014big}
\item DeepWalk: Online Learning of Social Representations~\cite{perozzi2014deepwalk}
\item Inductive Representation Learning on Large Graphs~\cite{hamilton2017inductive}
\end{itemize}

\section{Big Data Deep Learning: Challenges and Perspectives}
\subsection{Premise}
This work focuses on explaining how Deep Learning and Big Data are tied together
in various problems.
%
The elucidate the fact by explaining how data processing techniques from Big Data
may be used to improve the exexution efficiency of various Deep Learning mathods.
%
They focused majority of their efforts on Deep Belief Networks (DBNs), and Convolutional
Neural Networks (CNNs).

\subsection{Synthesis}
The authors explain ways in which Deep Learning Frameworks can be extended to use
modern hardware accelerators like GPUs to process data efficiently.
%
The inherant architecture of the GPUs make it very easy to model deep learning
algorithms for parallel execution.
%
GPUs allow for thousands of cores which allows convolutional operations to finish
much faster.
%
For making Deep Learning methods more suitable for big data processing the authors
present the following solutions:
\begin{itemize}
\item Large scale Deep Belief Networks
\item Large scale Convolutional Neural Networks
\item Combining Data- and Model-Parallel Schemes
\end{itemize}
Apart from proposing these solutions, the authors make readers explicitly aware of
challanges to look out for and explain their position on these challanges of applying
deep learning to big data, namely:
\begin{itemize}
\item Deep Learning for large volumes of data:
%
The authors believe that using GPU and CPU clusters is a good approach but using
data and model parallelism.
%
Inherent to deep learning is the challange of large number of parameters, and data
and model parallelism makes it easier to deal with such problems.
%
They also mention about the problems associated with large data which include
incomplete or noisy data.
%
\item Deep Learning for high variety of data
%
The authors mention about the problems of combining knowledge from various data
sources.
%
This could be done using data-integration.
%
They authors also survey recent works that demonstrate data integration in this
section.
%
\item Deep Learning for high velocity of data
%
To deal with this problem the authors discuss online learning.
%
This makes the model learn from one instance of data at a time.
%
Another challange of such data is the non-stationary nature.
%
The authors describe how data streaming may be able to help in this case.
\end{itemize}



\section{DeepWalk: Online Learning of Social Representations}
\subsection{Premise}
\subsection{Synthesis}

\section{Inductive Representation Learning on Large Graphs}
\subsection{Premise}
\subsection{Synthesis}

\bibliographystyle{plain}
\bibliography{report}

\end{document}

% chen2014big  perozzi2014deepwalk  hamilton2017inductive
